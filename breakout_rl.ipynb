{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train a Mario-playing RL Agent\n",
    "================\n",
    "\n",
    "Authors: `Yuansong Feng <https://github.com/YuansongFeng>`__, `Suraj\n",
    "Subramanian <https://github.com/suraj813>`__, `Howard\n",
    "Wang <https://github.com/hw26>`__, `Steven\n",
    "Guo <https://github.com/GuoYuzhang>`__.\n",
    "\n",
    "\n",
    "This tutorial walks you through the fundamentals of Deep Reinforcement\n",
    "Learning. At the end, you will implement an AI-powered Mario (using\n",
    "`Double Deep Q-Networks <https://arxiv.org/pdf/1509.06461.pdf>`__) that\n",
    "can play the game by itself.\n",
    "\n",
    "Although no prior knowledge of RL is necessary for this tutorial, you\n",
    "can familiarize yourself with these RL\n",
    "`concepts <https://spinningup.openai.com/en/latest/spinningup/rl_intro.html>`__,\n",
    "and have this handy\n",
    "`cheatsheet <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N>`__\n",
    "as your companion. The full code is available\n",
    "`here <https://github.com/yuansongFeng/MadMario/>`__.\n",
    "\n",
    ".. figure:: /_static/img/mario.gif\n",
    "   :alt: mario\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym-super-mario-bros==7.3.0\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version +978d2ce)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f73180fb0d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "def create_env():\n",
    "    env = gym.make('Breakout-v0')\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = GrayScaleObservation(env)\n",
    "    env = ResizeObservation(env, shape=84)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    return env\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "cpu = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "env = create_env()\n",
    "env.reset()\n",
    "\n",
    "for i in range(4):\n",
    "    next_state, reward, done, info = env.step(action=env.action_space.sample())\n",
    "# print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")\n",
    "frame1 = next_state[0]\n",
    "plt.imshow(frame1.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the above wrappers to the environment, the final wrapped\n",
    "state consists of 4 gray-scaled consecutive frames stacked together, as\n",
    "shown above in the image on the left. Each time Mario makes an action,\n",
    "the environment responds with a state of this structure. The structure\n",
    "is represented by a 3-D array of size ``[4, 84, 84]``.\n",
    "\n",
    ".. figure:: /_static/img/mario_env.png\n",
    "   :alt: picture\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "We create a class ``Mario`` to represent our agent in the game. Mario\n",
    "should be able to:\n",
    "\n",
    "-  **Act** according to the optimal action policy based on the current\n",
    "   state (of the environment).\n",
    "\n",
    "-  **Remember** experiences. Experience = (current state, current\n",
    "   action, reward, next state). Mario *caches* and later *recalls* his\n",
    "   experiences to update his action policy.\n",
    "\n",
    "-  **Learn** a better action policy over time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = Net(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "        self.save_index = 0\n",
    "\n",
    "        self.save_every = 500000\n",
    "        \n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "        Inputs:\n",
    "        state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "        Outputs:\n",
    "        action_idx (int): An integer representing which action Mario will perform\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "        state = torch.tensor(state).to(cpu)\n",
    "        next_state = torch.tensor(next_state).to(cpu)\n",
    "        action = torch.tensor([action]).to(cpu)\n",
    "        reward = torch.tensor([reward]).to(cpu)\n",
    "        done = torch.tensor([done]).to(cpu)\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state.to(device), next_state.to(device), action.squeeze().to(device), reward.squeeze().to(device), done.squeeze().to(device)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will populate Mario’s parameters and\n",
    "define his functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn\n",
    "--------------\n",
    "\n",
    "Mario uses the `DDQN algorithm <https://arxiv.org/pdf/1509.06461>`__\n",
    "under the hood. DDQN uses two ConvNets - $Q_{online}$ and\n",
    "$Q_{target}$ - that independently approximate the optimal\n",
    "action-value function.\n",
    "\n",
    "In our implementation, we share feature generator ``features`` across\n",
    "$Q_{online}$ and $Q_{target}$, but maintain separate FC\n",
    "classifiers for each. $\\theta_{target}$ (the parameters of\n",
    "$Q_{target}$) is frozen to prevent updation by backprop. Instead,\n",
    "it is periodically synced with $\\theta_{online}$ (more on this\n",
    "later).\n",
    "\n",
    "Neural Network\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"mini cnn structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD Estimate & TD Target\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Two values are involved in learning:\n",
    "\n",
    "**TD Estimate** - the predicted optimal $Q^*$ for a given state\n",
    "$s$\n",
    "\n",
    "\\begin{align}{TD}_e = Q_{online}^*(s,a)\\end{align}\n",
    "\n",
    "**TD Target** - aggregation of current reward and the estimated\n",
    "$Q^*$ in the next state $s'$\n",
    "\n",
    "\\begin{align}a' = argmax_{a} Q_{online}(s', a)\\end{align}\n",
    "\n",
    "\\begin{align}{TD}_t = r + \\gamma Q_{target}^*(s',a')\\end{align}\n",
    "\n",
    "Because we don’t know what next action $a'$ will be, we use the\n",
    "action $a'$ maximizes $Q_{online}$ in the next state\n",
    "$s'$.\n",
    "\n",
    "Notice we use the\n",
    "`@torch.no_grad() <https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad>`__\n",
    "decorator on ``td_target()`` to disable gradient calculations here\n",
    "(because we don’t need to backpropagate on $\\theta_{target}$).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the model\n",
    "~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "As Mario samples inputs from his replay buffer, we compute $TD_t$\n",
    "and $TD_e$ and backpropagate this loss down $Q_{online}$ to\n",
    "update its parameters $\\theta_{online}$ ($\\alpha$ is the\n",
    "learning rate ``lr`` passed to the ``optimizer``)\n",
    "\n",
    "\\begin{align}\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\\end{align}\n",
    "\n",
    "$\\theta_{target}$ does not update through backpropagation.\n",
    "Instead, we periodically copy $\\theta_{online}$ to\n",
    "$\\theta_{target}$\n",
    "\n",
    "\\begin{align}\\theta_{target} \\leftarrow \\theta_{online}\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save checkpoint\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"net_{self.save_index}.p\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), optim=self.optimizer.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        self.save_index += 1\n",
    "        print(f\"Net saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 50000  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 10000  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging\n",
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s play!\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "In this example we run the training loop for 10 episodes, but for Mario to truly learn the ways of\n",
    "his world, we suggest running the loop for at least 40,000 episodes!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n",
      "Episode 0 - Step 113 - Epsilon 0.9999717503954925 - Mean Reward 5.0 - Mean Length 113.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.242 - Time 2021-12-25T21:57:34\n",
      "Episode 10 - Step 865 - Epsilon 0.9997837733532894 - Mean Reward 1.636 - Mean Length 78.636 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.612 - Time 2021-12-25T21:57:35\n",
      "Episode 20 - Step 1553 - Epsilon 0.9996118253107109 - Mean Reward 1.524 - Mean Length 73.952 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.48 - Time 2021-12-25T21:57:37\n",
      "Episode 30 - Step 2378 - Epsilon 0.9994056766057594 - Mean Reward 1.742 - Mean Length 76.71 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.776 - Time 2021-12-25T21:57:38\n",
      "Episode 40 - Step 2973 - Epsilon 0.9992570260489214 - Mean Reward 1.488 - Mean Length 72.512 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.298 - Time 2021-12-25T21:57:40\n",
      "Episode 50 - Step 3768 - Epsilon 0.9990584434249461 - Mean Reward 1.588 - Mean Length 73.882 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.919 - Time 2021-12-25T21:57:42\n",
      "Episode 60 - Step 4666 - Epsilon 0.9988341799508502 - Mean Reward 1.754 - Mean Length 76.492 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.938 - Time 2021-12-25T21:57:44\n",
      "Episode 70 - Step 5483 - Epsilon 0.9986301888773655 - Mean Reward 1.761 - Mean Length 77.225 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.828 - Time 2021-12-25T21:57:45\n",
      "Episode 80 - Step 6311 - Epsilon 0.9984234937960808 - Mean Reward 1.802 - Mean Length 77.914 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.82 - Time 2021-12-25T21:57:47\n",
      "Episode 90 - Step 7072 - Epsilon 0.9982335617704732 - Mean Reward 1.758 - Mean Length 77.714 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.689 - Time 2021-12-25T21:57:49\n",
      "Episode 100 - Step 7778 - Epsilon 0.9980573890724722 - Mean Reward 1.66 - Mean Length 76.65 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.578 - Time 2021-12-25T21:57:51\n",
      "Episode 110 - Step 8517 - Epsilon 0.9978730149788491 - Mean Reward 1.65 - Mean Length 76.52 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.673 - Time 2021-12-25T21:57:52\n",
      "Episode 120 - Step 9263 - Epsilon 0.9976869289913254 - Mean Reward 1.69 - Mean Length 77.1 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.7 - Time 2021-12-25T21:57:54\n",
      "Episode 130 - Step 9925 - Epsilon 0.9975218254466117 - Mean Reward 1.56 - Mean Length 75.47 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.505 - Time 2021-12-25T21:57:55\n",
      "Episode 140 - Step 10702 - Epsilon 0.9973280756263341 - Mean Reward 1.69 - Mean Length 77.29 - Mean Loss 0.001 - Mean Q Value -0.002 - Time Delta 4.11 - Time 2021-12-25T21:58:00\n",
      "Episode 150 - Step 11346 - Epsilon 0.9971675187112444 - Mean Reward 1.57 - Mean Length 75.78 - Mean Loss 0.002 - Mean Q Value -0.004 - Time Delta 3.55 - Time 2021-12-25T21:58:03\n",
      "Episode 160 - Step 12095 - Epsilon 0.9969808165505375 - Mean Reward 1.45 - Mean Length 74.29 - Mean Loss 0.003 - Mean Q Value -0.006 - Time Delta 4.128 - Time 2021-12-25T21:58:07\n",
      "Episode 170 - Step 12936 - Epsilon 0.9967712233418873 - Mean Reward 1.51 - Mean Length 74.53 - Mean Loss 0.004 - Mean Q Value -0.008 - Time Delta 5.286 - Time 2021-12-25T21:58:13\n",
      "Episode 180 - Step 13725 - Epsilon 0.9965746295831778 - Mean Reward 1.47 - Mean Length 74.14 - Mean Loss 0.005 - Mean Q Value -0.01 - Time Delta 4.383 - Time 2021-12-25T21:58:17\n",
      "Episode 190 - Step 14514 - Epsilon 0.9963780745987687 - Mean Reward 1.5 - Mean Length 74.42 - Mean Loss 0.006 - Mean Q Value -0.012 - Time Delta 4.191 - Time 2021-12-25T21:58:21\n",
      "Episode 200 - Step 15206 - Epsilon 0.9962057160797358 - Mean Reward 1.52 - Mean Length 74.28 - Mean Loss 0.007 - Mean Q Value -0.014 - Time Delta 3.822 - Time 2021-12-25T21:58:25\n",
      "Episode 210 - Step 16049 - Epsilon 0.9959957878207688 - Mean Reward 1.61 - Mean Length 75.32 - Mean Loss 0.008 - Mean Q Value -0.015 - Time Delta 4.505 - Time 2021-12-25T21:58:29\n",
      "Episode 220 - Step 16825 - Epsilon 0.9958025833551933 - Mean Reward 1.6 - Mean Length 75.62 - Mean Loss 0.008 - Mean Q Value -0.016 - Time Delta 4.306 - Time 2021-12-25T21:58:34\n",
      "Episode 230 - Step 17565 - Epsilon 0.9956183768938448 - Mean Reward 1.62 - Mean Length 76.4 - Mean Loss 0.009 - Mean Q Value -0.018 - Time Delta 3.768 - Time 2021-12-25T21:58:37\n",
      "Episode 240 - Step 18155 - Epsilon 0.9954715339948071 - Mean Reward 1.48 - Mean Length 74.53 - Mean Loss 0.008 - Mean Q Value -0.017 - Time Delta 3.152 - Time 2021-12-25T21:58:41\n",
      "Episode 250 - Step 18845 - Epsilon 0.9952998299436056 - Mean Reward 1.53 - Mean Length 74.99 - Mean Loss 0.007 - Mean Q Value -0.017 - Time Delta 3.738 - Time 2021-12-25T21:58:44\n",
      "Episode 260 - Step 19617 - Epsilon 0.9951077555881616 - Mean Reward 1.57 - Mean Length 75.22 - Mean Loss 0.007 - Mean Q Value -0.017 - Time Delta 4.372 - Time 2021-12-25T21:58:49\n",
      "Episode 270 - Step 20391 - Epsilon 0.9949152208416991 - Mean Reward 1.5 - Mean Length 74.55 - Mean Loss 0.006 - Mean Q Value -0.015 - Time Delta 3.963 - Time 2021-12-25T21:58:53\n",
      "Episode 280 - Step 21167 - Epsilon 0.9947222259858112 - Mean Reward 1.49 - Mean Length 74.42 - Mean Loss 0.006 - Mean Q Value -0.012 - Time Delta 4.003 - Time 2021-12-25T21:58:57\n",
      "Episode 290 - Step 21951 - Epsilon 0.9945272795105006 - Mean Reward 1.46 - Mean Length 74.37 - Mean Loss 0.005 - Mean Q Value -0.009 - Time Delta 4.092 - Time 2021-12-25T21:59:01\n",
      "Episode 300 - Step 22691 - Epsilon 0.9943433089585693 - Mean Reward 1.49 - Mean Length 74.85 - Mean Loss 0.004 - Mean Q Value -0.007 - Time Delta 3.919 - Time 2021-12-25T21:59:05\n",
      "Episode 310 - Step 23543 - Epsilon 0.9941315363618412 - Mean Reward 1.52 - Mean Length 74.94 - Mean Loss 0.004 - Mean Q Value -0.005 - Time Delta 4.413 - Time 2021-12-25T21:59:09\n",
      "Episode 320 - Step 24165 - Epsilon 0.9939769609071468 - Mean Reward 1.42 - Mean Length 73.4 - Mean Loss 0.004 - Mean Q Value -0.003 - Time Delta 3.451 - Time 2021-12-25T21:59:13\n",
      "Episode 330 - Step 24880 - Epsilon 0.9937993033817665 - Mean Reward 1.44 - Mean Length 73.15 - Mean Loss 0.004 - Mean Q Value -0.001 - Time Delta 3.774 - Time 2021-12-25T21:59:16\n",
      "Episode 340 - Step 25675 - Epsilon 0.9936018053725192 - Mean Reward 1.57 - Mean Length 75.2 - Mean Loss 0.004 - Mean Q Value 0.002 - Time Delta 4.23 - Time 2021-12-25T21:59:21\n",
      "Episode 350 - Step 26376 - Epsilon 0.9934276918914794 - Mean Reward 1.57 - Mean Length 75.31 - Mean Loss 0.004 - Mean Q Value 0.004 - Time Delta 4.209 - Time 2021-12-25T21:59:25\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"/local/knagaitsev/breakout_rl\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "agent = Agent(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        agent.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = agent.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        logger.record(episode=e, epsilon=agent.exploration_rate, step=agent.curr_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "In this tutorial, we saw how we can use PyTorch to train a game-playing AI. You can use the same methods\n",
    "to train an AI to play any of the games at the `OpenAI gym <https://gym.openai.com/>`__. Hope you enjoyed this tutorial, feel free to reach us at\n",
    "`our github <https://github.com/yuansongFeng/MadMario/>`__!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "env = create_env()\n",
    "mario.exploration_rate = 1.0\n",
    "\n",
    "for e in range(1):\n",
    "\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        time.sleep(0.01)\n",
    "        action = mario.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print(next_state.shape)\n",
    "        break\n",
    "        state = next_state\n",
    "\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
